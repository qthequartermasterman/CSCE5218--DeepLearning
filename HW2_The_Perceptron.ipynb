{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYiZq0X2oB5t"
   },
   "source": [
    "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
    "\n",
    "# **HW1a The Perceptron** (20 pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGVmKzgG2Ium",
    "outputId": "4cc2ca21-861a-4fba-a38c-83e3ec04bec8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-31 19:36:55--  http://huang.eng.unt.edu/CSCE-5218/test.dat\r\n",
      "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\r\n",
      "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 2844 (2.8K)\r\n",
      "Saving to: ‘test.dat.1’\r\n",
      "\r\n",
      "test.dat.1          100%[===================>]   2.78K  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2022-01-31 19:36:55 (5.95 MB/s) - ‘test.dat.1’ saved [2844/2844]\r\n",
      "\r\n",
      "--2022-01-31 19:36:55--  http://huang.eng.unt.edu/CSCE-5218/train.dat\r\n",
      "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\r\n",
      "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 11244 (11K)\r\n",
      "Saving to: ‘train.dat.1’\r\n",
      "\r\n",
      "train.dat.1         100%[===================>]  10.98K  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2022-01-31 19:36:55 (43.6 MB/s) - ‘train.dat.1’ saved [11244/11244]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Get the datasets\n",
    "!wget http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
    "!wget http://huang.eng.unt.edu/CSCE-5218/train.dat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A69DxPSc8vNs",
    "outputId": "5440e602-8ecd-44cf-d48d-2e8b00cdcc52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\t\r\n",
      "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\r\n",
      "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\r\n",
      "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\r\n",
      "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\r\n",
      "0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\r\n",
      "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\r\n",
      "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\r\n",
      "0\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\r\n",
      "0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t0\t1\t0\r\n",
      "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\r\n",
      "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\r\n",
      "0\t0\t0\t1\t0\t0\t1\t1\t0\t1\t0\t0\t1\t0\r\n",
      "0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\r\n",
      "0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t0\r\n",
      "0\t1\t0\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t0\r\n",
      "0\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\r\n",
      "0\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\r\n",
      "0\t1\t0\t0\t1\t0\t0\t1\t1\t0\t1\t1\t1\t0\r\n",
      "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\r\n"
     ]
    }
   ],
   "source": [
    "# Take a peek at the datasets\n",
    "!head train.dat\n",
    "!head test.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFXHLhnhwiBR"
   },
   "source": [
    "### Build the Perceptron Model\n",
    "\n",
    "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cXAsP_lw3QwJ"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "\n",
    "# Corpus reader, all columns but the last one are coordinates;\n",
    "#   the last column is the label\n",
    "def read_data(file_name):\n",
    "    # NOTE: I had to adjust the implementation of this function in order for the code below to run.\n",
    "    f = open(file_name, 'r')\n",
    "\n",
    "    data = []\n",
    "    # Discard header line\n",
    "    f.readline()\n",
    "    for instance in f.readlines():\n",
    "        if not re.search('\\t', instance): continue\n",
    "        # instance = [list(map(int, instance.strip().split('\\t')))]\n",
    "        instance = list(map(int, instance.strip().split('\\t')))\n",
    "        # Add a dummy input so that w0 becomes the bias\n",
    "        instance = [-1] + instance\n",
    "        # data += instance\n",
    "        data.append(instance)\n",
    "    return data\n",
    "\n",
    "\n",
    "def dot_product(array1, array2):\n",
    "    \"\"\"\n",
    "    The dot product is the sum of pairwise products between two vectors/arrays.\n",
    "    :param array1: some array of (real) numerical data\n",
    "    :param array2: some array of (real) numerical data\n",
    "    :return: number representing the dot product of the two arrays\n",
    "    \"\"\"\n",
    "    return sum(x*y for x,y in zip(array1, array2))\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    The sigmoid function is a smooth, nonlinear activation function that restricts the real line to a range of (0,1).\n",
    "    The formula of the sigmoid function is `1/(1+math.exp(-x))`.\n",
    "    :param x: some number in the real numbers\n",
    "    :return: the number equal to Sigmoid(x) in the range (0,1)\n",
    "    \"\"\"\n",
    "    return 1/(1+math.exp(-x))\n",
    "\n",
    "# The output of the model, which for the perceptron is \n",
    "# the sigmoid function applied to the dot product of \n",
    "# the instance and the weights\n",
    "def output(weight, instance):\n",
    "    \"\"\"\n",
    "    A Perceptron is the composition of the dot product of weight and instance and sigmoid.\n",
    "    :param weight: some array of weight values.\n",
    "    :param instance: some array of input values\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return sigmoid(dot_product(weight, instance))\n",
    "\n",
    "# Predict the label of an instance; this is the definition of the perceptron\n",
    "# you should output 1 if the output is >= 0.5 else output 0\n",
    "def predict(weights, instance):\n",
    "    \"\"\"\n",
    "    A prediction is 1 (True) if the output of the perceptron is greater than some threshold (0.5). 0 (False) otherwise.\n",
    "    :param weight: some array of weight values.\n",
    "    :param instance: some array of input values\n",
    "    :return: 1 or 0 depending on the value of the output\n",
    "    \"\"\"\n",
    "    return int(output(weights, instance) >= .5)\n",
    "\n",
    "\n",
    "# Accuracy = percent of correct predictions\n",
    "def get_accuracy(weights, instances):\n",
    "    # You do not to write code like this, but get used to it\n",
    "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
    "                   for instance in instances])\n",
    "    return correct * 100 / len(instances)\n",
    "\n",
    "\n",
    "# Train a perceptron with instances and hyperparameters:\n",
    "#       lr (learning rate) \n",
    "#       epochs\n",
    "# The implementation comes from the definition of the perceptron\n",
    "#\n",
    "# Training consists on fitting the parameters which are the weights\n",
    "# that's the only thing training is responsible to fit\n",
    "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
    "#\n",
    "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
    "# We are updating weights in the opposite direction of the gradient of the error,\n",
    "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
    "def train_perceptron(instances, lr, epochs):\n",
    "    # NOTE: I had to adjust the implementation of this function in order for the code below to run.\n",
    "\n",
    "    # Initialize the weights\n",
    "    # weights = [0] * (len(instances[0])-1)\n",
    "    weights = [0] * (len(instances[0]))\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for instance in instances:\n",
    "            # Forward propagation (evaluate the model on this data point)\n",
    "            in_value = dot_product(weights, instance)\n",
    "            output = sigmoid(in_value)\n",
    "            output = predict(weights, instance)\n",
    "            error = instance[-1] - output\n",
    "            # Back propagation (adjust the weights by stepping in the direction of the negative gradient\n",
    "            for i in range(len(weights)):\n",
    "                weights[i] += lr * error * output * (1-output) * instance[i]\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adBZuMlAwiBT"
   },
   "source": [
    "## Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "50YvUza-BYQF"
   },
   "outputs": [],
   "source": [
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "# instances_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n"
     ]
    }
   ],
   "source": [
    "lr = 0.005\n",
    "epochs = 5\n",
    "weights = train_perceptron(instances_tr, lr, epochs)\n",
    "accuracy = get_accuracy(weights, instances_te)\n",
    "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBXkvaiQMohX"
   },
   "source": [
    "## Questions\n",
    "\n",
    "Answer the following questions. Include your implementation and the output for each question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCQ6BEk1CBlr"
   },
   "source": [
    "\n",
    "\n",
    "### Question 1\n",
    "\n",
    "In `train_perceptron(instances, lr, epochs)`, we have the following code:\n",
    "```\n",
    "in_value = dot_product(weights, instance)\n",
    "output = sigmoid(in_value)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "Why don't we have the following code snippet instead?\n",
    "```\n",
    "output = predict(weights, instance)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "#### TODO Add your answer here (text only)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Question 1 Answer\n",
    "The function `predict` does more than just calculate the `dot_product` and apply `sigmoid`. It also checks if the\n",
    "`output` is higher than the threshold of 0.5 and returns a `0` or `1`. Running a training loop with the second code\n",
    "snippet reveals immediately that this is a problem:\n",
    "```\n",
    "#tr: 20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
    "#tr: 20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
    "#tr: 20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
    "#tr: 20, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
    "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
    "```\n",
    "Clearly, the modified inference that uses the prediction directly does not learn anything! This is because of the way\n",
    "back-propagation is done. Consider the second code snippet with the following back-propagation step.\n",
    "```\n",
    "output = predict(weights, instance)\n",
    "error = instance[-1] - output\n",
    "# Back propagation (adjust the weights by stepping in the direction of the negative gradient\n",
    "for i in range(len(weights)):\n",
    "    weights[i] += lr * error * output * (1-output) * instance[i]\n",
    "```\n",
    "In this case `output = 1` or `output=0`. In either case the term `output * (1-output)` equals `0`. Thus, this step size\n",
    "is constantly zero, and the model learns nothing each iteration. Training against non-binary outputs allows this step to\n",
    " have variable, nonzero size.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JU3c3m6YL2rK"
   },
   "source": [
    "### Question 2\n",
    "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
    "\n",
    "```\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
    "lr = [0.005, 0.01, 0.05]              # learning rate\n",
    "```\n",
    "\n",
    "TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)\n",
    "of your code.The output should look like the following:\n",
    "```\n",
    "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "[and so on for all the combinations]\n",
    "```\n",
    "You will get different results with different hyperparameters.\n",
    "\n",
    "### Question 2 Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "G-VKJOUu2BTp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr: 20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
    "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
    "\n",
    "for lr in lr_array:\n",
    "    for tr_size in tr_percent:\n",
    "        for epochs in num_epochs:\n",
    "            size =  round(len(instances_tr)*tr_size/100)\n",
    "            pre_instances = instances_tr[0:size]\n",
    "            weights = train_perceptron(pre_instances, lr, epochs)\n",
    "            accuracy = get_accuracy(weights, instances_te)\n",
    "            print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "                    f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFB9MtwML24O"
   },
   "source": [
    "### Question 3\n",
    "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
    "- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
    "- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
    "   ```\n",
    "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
    "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "```\n",
    "- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
    "- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
    "\n",
    "#### TODO: Add your answer here (code and text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38rA_Kp3wiBX"
   },
   "source": [
    "### Question 3 Answers\n",
    "- **A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?**\n",
    "\n",
    "No. You do not need to train with all the training dataset to get the highest accuracy.\n",
    "As long as the subset of the training data is relatively representative of the entire dataset, then training will\n",
    "accomplish the same result. Sometimes, when other hyperparameters are adjusted, an experiment with less data can even\n",
    "achieve higher final accuracy.\n",
    "Consider for example these two trials from the above set:\n",
    "\n",
    "```\n",
    "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
    "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 91.0\n",
    "```\n",
    "\n",
    "Clearly, the second trial here (with 20 data points) achieves higher accuracy than the other (with 40 data points).\n",
    "\n",
    "- B. **How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?**\n",
    "   ```\n",
    "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
    "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "```\n",
    "\n",
    "In the second example, the learning rate is much lower. This learning rate is proportional to the \"step size\" in each\n",
    "iteration. Thus, with a lower step size, the model will iterate through parameter space more slowly, meaning it will\n",
    "find a local minimum much slower and training will require more steps to achieve the same accuracy. Since both\n",
    "experiments only had 20 epochs at this measurement (and 20 epochs is not very long into training), the second experiment\n",
    "would have traversed less distance, and thus is farther from a local minimum in the parameter space.\n",
    "\n",
    "- C. **Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?**\n",
    "Yes. Adjusting the hyperparameters can achieve higher than 80% accuracy. Consider this experiment:\n",
    "```\n",
    "#tr: 400, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
    "```\n",
    "Clearly, the accuracy is 100% on the sampled test data, which is higher than 80% accuracy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- D. **Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?**\n",
    "Sometimes, additional training yields diminishing returns. Consider for example this training loop:\n",
    "```\n",
    "#tr: 20, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 20, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
    "#tr: 20, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 85.0\n",
    "#tr: 20, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 91.0\n",
    "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 91.0\n",
    "```\n",
    "In this loop, we see that the perceptron learns quickly at first, but the rate at which accuracy improves decreases\n",
    "over time. In fact, the model is no more accurate after 100 epochs than it is at 50 epochs. This means that even with\n",
    "the same hyperparameters, the model converged to its final accuracy around 50 epochs. Any additional training after that\n",
    " point accomplished no improved accuracy, and thus additional training is not worth the effort."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW2_The_Perceptron.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}